




<diverse>
Search test:
- "message appears" - viste external media som IKKE var indekseret!?
- external media respect privacy of pages?
- external media on multiple pages with DIFFERENT languages?
- checkResume: Should it also check for gr_list "0,-1"?
- case-sensitivity?
- Warning: phash-row "114682730" didn't have a representation in the index_section table! on references page! (typo3site_live)
Example på alternativ søgnings SQL
XHTML i frontend?

Backend modules:
- Proper skinning? / getLL? / XHTML

Implement stop-word setting in: ""Top-20 words by count:" and a list seperate from that (in main module?)
- Display filters:
	- The checkbox "No Search" in the page header is only respected by indexed_search during indexing! (A page will not be indexed when "No Search" is set). However when searching results are not filtered based on this flag - so if a page is indexed before the no search flag is set it will be found in search results. To change this is hard because the getTreeList() function that fetches all page ids cannot take a where-clause to filter it out but must have hardcoded support. Alternatively the pages table must be joined into the search result so we can select on the field. A solution is still not agreed upon.
	- For tt_news with access restricted records: don't show the title of page since it can reveal information
		SOLUTIONS: Maybe just hide search results where "resume" is normally just not shown?
</diverse>

Unittest for t3lib_cs converting Euc/shift_jis

- Søgning i tabeller (reg. tabeller!)
	- Alternative presentationer af når records er indexerede.
	- incl. meta-data?
	- Tabelvælger som en del af sektionsvælgeren
- Support for visning i extra niveaer?




Test kaniner (indexed search / caching?):
	- 3DS
	- TYPO3.org copy
	- Metropol
	- FI
	- Link Factory
	- Brunata

**************
getLL with XML-support?


CLI:
- Removal of old indexes
	- delete results with large tstamp (thats all....)
- Indexing configurations
	- (Indexing of records from tables should be done automatically in TCEmain with a hook for create/update/delete)
	- Look up all index configurations
	- Look up phash records (field tstamp) based on config-uid
		- For files: read files from directories, compare mtime with records;
		- For URLs: Forced
		- For records: read records
			- All new files are indexed, all old are removed, all changed are re-indexed


CRAWLER:
Purpose: To request URLs

Main components:

Page tree module:
- Purger:
	- On page id/tree depth: creating URLs from parameters, inserting those URLs in crawler-queue, setting a tstamp which distributes requests over time.
	- Backend module
		- shows for each parameter line the calculated URLs before submitting them (each parameter in a column showing values or count of UIDs for tables)
		- Starttime can be set.
		- Alternative: Export URLs to script that WGET can run?
	- CLI enables this to be done periodically, walking the page tree.
- Crawler:
	- Looking up URLs from queue, sending requests based on that, storing result in log (linked to unique ID for queue entry) (removing log entries if more than 10...)
	- Backend module
		- shows queue entries with log entries as well
		- can initiate a request by force of course
	- CLI enables automatic crawling

Root level in page tree:
	- Showing overall log / status.
	- Flush log

Frontend link generator can also store a URL in the queue!
	- Kun cachable URLs
		- Either standard params like MP / id / type or cHashed params

crawlerCfg {
	procInstructions = ...
	clientIdentity = msie etc....
	userGroupLists = [0,-1|1,2,3|4]
	paramSets {
			# Parameters should be url-encoded already:
			# Special tokens are: ";:,-"
		100 = &L=[|1|2]		// Three values for language: "", "1" and "2"
		110 = &myext[uid]=[1-34|35|36-10]&another=[1|2|N%20A]		// Creating URls with the ranges of integers combined with three positions of "another" which are "1", "2" and "N A"
		120 = &print=1
		130 = &=[_TABLE:sys_language; _PID:0 (optional, default is current id)]&L=[|1|2]		// If no records are selected from table, no URLs are generated
		130.pidsOnly = 123,456
		130.procInstructions = RECACHE, INDEX, PUBLISH
		130.clientIdentity = msie etc....
		140.userGroupLists = [0,-1|1,2,3|4]
	}
}

Request:
- Header has "X-T3crawler : [qid]:[md5()-hash of qid combined with encryptionKey]
- Header has "X-T3crawler-ProcInstruction : ..."
- Header has "X-T3crawler-UserGroup : ..."
- Request is authenticated by looking up qid and calculating hash. (might include IP lock)
- If OK, then the result is info-array rather than HTML output.


Queue:
	qid	(int hash of page_id / parameters are unique id / procIn / clientID / usergroup lists) - PRIMARY KEY
	page_id		(if zero: Function call , see special instructioncs)
	parameters	(or serialized parameters for function call)
	domain
	special_instructions 	(function call)
	starttime 	// Scheduled to being processed at this point
	exec_time	// Zero when not done, otherwise set to execution time. This will help us to spot "outdated" entries by time since this

When a URL is reindexed the SAME record is used (qid), starttime set and exec_time reset
Log entries are kept

Log:
	uid	// Autoincr.
	qid		// Queue ID
	result	// serialized data
	exec_time
	- cached?
	- parsetime?
	- strlen?
	- messages?




***************
TODO / projects:
*****************

Documentation:
- Configuration possibilities (piVars, TypoScript, Hooks etc)
- How to setup up, analyse and debug indexed search (manual)
- Technical:
	- utf-8 internally.
	- Updates on tables structure

Statistics module:
- Someone write a statistics display module for the search operation! (Displaying content from index_stat_search and index_stat_word)
	Olivier Dobberkau / dkd is on this.

Various:
- The Tools>Indexing module could need some shining up and more useful features (Someone else does this?)

Templating / Display in plugin:
- Templating
	- with new Template API?
	- Still need to put a group together.

